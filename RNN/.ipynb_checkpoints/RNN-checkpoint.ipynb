{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Math / Data libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data Scrape Package\n",
    "import pandas_datareader.data as web\n",
    "\n",
    "# Plotting package\n",
    "import matplotlib.pyplot as plt\n",
    "# Scaling Package\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Keras Network @ https://www.tensorflow.org/guide/keras/rnn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set Random seed\n",
    "import random\n",
    "random.seed(2505)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Data on Gold.\n",
    "tick = 'GC=F'\n",
    "gold = web.get_data_yahoo(tick,'12/20/2015',interval='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gold['Adj Close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = gold['Adj Close'].pct_change() # Used for univariate example.\n",
    "\n",
    "column_names = gold.columns\n",
    "x = gold.values #returns a numpy array\n",
    "min_max_scaler = MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_c_gold = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_c_gold.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_c_gold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Forecasting (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten this matrix down.\n",
    "npa = returns.values[1:].reshape(-1,1) # Python is smart to recognize whatever dimension you need by using this parameter\n",
    "print(len(npa))\n",
    "# # Let's scale the data -- this helps avoid the exploding gradient issue\n",
    "scale = MinMaxScaler(feature_range=(0,1)) # This is by default.\n",
    "npa = scale.fit_transform(npa)\n",
    "print(len(npa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need the data to be in the form [sample, time steps, features (dimension of each element)]\n",
    "samples = 10 # Number of samples (in past)\n",
    "steps = 1 # Number of steps (in future)\n",
    "X = [] # X array\n",
    "Y = [] # Y array\n",
    "for i in range(npa.shape[0] - samples):\n",
    "    X.append(npa[i:i+samples]) # Independent Samples\n",
    "    Y.append(npa[i+samples][0]) # Dependent Samples\n",
    "print('Training Data: Length is ',len(X[0:1][0]),': ', X[0:1])\n",
    "print('Testing Data: Length is ', len(Y[0:1]),': ', Y[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape the data so that the inputs will be acceptable to the model.\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "print('Dimensions of X', X.shape, 'Dimensions of Y', Y.shape)\n",
    "\n",
    "threshold = round(0.9 * X.shape[0])\n",
    "print('Threshold is', threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build the RNN\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add a RNN layer with 3 units.\n",
    "model.add(layers.SimpleRNN(3,\n",
    "                           activation = 'tanh',\n",
    "                           use_bias=True,\n",
    "                           input_shape=(X.shape[1], X.shape[2])))\n",
    "# Add a dropout layer (penalizing more complex models) -- prevents overfitting\n",
    "model.add(layers.Dropout(rate=0.2))\n",
    "\n",
    "\n",
    "# Add a Dense layer with 1 units (Since we are doing a regression task.\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "# Evaluating loss function of MSE using the adam optimizer.\n",
    "model.compile(loss='mean_squared_error', optimizer = 'adam')\n",
    "\n",
    "# Print out architecture.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the data\n",
    "history = model.fit(X[:threshold],\n",
    "                    Y[:threshold],\n",
    "                    shuffle = False, # Since this is time series data\n",
    "                    epochs=100,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1) # Verbose outputs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss iteration\n",
    "plt.plot(history.history['loss'], label = 'training loss')\n",
    "plt.plot(history.history['val_loss'], label ='validation loss')\n",
    "plt.legend()\n",
    "# Note:\n",
    "# if training loss >> validation loss -> Underfitting\n",
    "# if training loss << validation loss -> Overfitting (i.e model is smart enough to have mapped the entire dataset..)\n",
    "# Several ways to address overfitting:\n",
    "# Reduce complexity of model (hidden layers, neurons, parameters input etc)\n",
    "# Add dropout and tune rate\n",
    "# More data :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Step Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions using the 'sliding/rolling window approach'\n",
    "# Multi step forecast.\n",
    "# Using self predictions for making future predictions\n",
    "\n",
    "true_Y = Y[threshold:]\n",
    "pred_Y = []\n",
    "print('Number of Forecasts to do: ',Y.shape[0] - round(Y.shape[0]*0.9))\n",
    "latest_input = X[threshold-1:threshold]\n",
    "for i in range(Y.shape[0] - round(Y.shape[0]*0.9) ):\n",
    "    # Prediction\n",
    "    p = model.predict(latest_input.reshape(1,X.shape[1],1))[0,0]\n",
    "    \n",
    "    # Update predictions\n",
    "    pred_Y.append(p)\n",
    "    latest_input = np.append(X[threshold][1:], p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(true_Y, label = 'True Value')\n",
    "plt.plot(pred_Y, label = 'Forecasted Value')\n",
    "plt.legend()\n",
    "# Model just copied the same value over and over again. Hence, model is not very robust.\n",
    "# It's just predicting the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_c_gold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Variate Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using our Training and Testing sets, Let's create our inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need the data to be in the form [sample, time steps, features (dimension of each element)]\n",
    "samples = 10 # Number of samples (in past)\n",
    "steps = 1 # Number of steps (in future)\n",
    "X = [] # X array\n",
    "Y = [] # Y array\n",
    "for i in range(pct_c_gold.shape[0] - samples):\n",
    "    X.append(pct_c_gold.iloc[i:i+samples, 0:5].values) # Independent Samples\n",
    "    Y.append(pct_c_gold.iloc[i+samples, 5:].values) # Dependent Samples\n",
    "print('Training Data: Length is ',len(X[0:1][0]),': ', X[0:1])\n",
    "print('Testing Data: Length is ', len(Y[0:1]),': ', Y[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape the data so that the inputs will be acceptable to the model.\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "print('Dimensions of X', X.shape, 'Dimensions of Y', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the training and testing set\n",
    "threshold = round(0.9 * X.shape[0])\n",
    "trainX, trainY = X[:threshold], Y[:threshold]\n",
    "testX, testY =  X[threshold:], Y[threshold:]\n",
    "print('Training Length',trainX.shape, trainY.shape,'Testing Length:',testX.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build the RNN\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add a RNN layer with 30 internal units.\n",
    "model.add(layers.SimpleRNN(30,\n",
    "                           activation = 'tanh',\n",
    "                           use_bias=True,\n",
    "                           input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "# Add a dropout layer (penalizing more complex models) -- prevents overfitting\n",
    "model.add(layers.Dropout(rate=0.2))\n",
    "\n",
    "\n",
    "# Add a Dense layer with 1 units (Since we are doing a regression task.\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "# Evaluating loss function of MSE using the adam optimizer.\n",
    "model.compile(loss='mean_squared_error', optimizer = 'adam')\n",
    "\n",
    "# Print out architecture.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the data\n",
    "history = model.fit(trainX,\n",
    "                    trainY,\n",
    "                    shuffle = False, # Since this is time series data\n",
    "                    epochs=100,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1) # Verbose outputs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss iteration\n",
    "plt.plot(history.history['loss'], label = 'training loss')\n",
    "plt.plot(history.history['val_loss'], label ='validation loss')\n",
    "plt.legend()\n",
    "# Note:\n",
    "# if training loss >> validation loss -> Underfitting\n",
    "# if training loss << validation loss -> Overfitting (i.e model is smart enough to have mapped the entire dataset..)\n",
    "# Several ways to address overfitting:\n",
    "# Reduce complexity of model (hidden layers, neurons, parameters input etc)\n",
    "# Add dropout and tune rate\n",
    "# More data :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a one step forecast (based on how we constructed our model)\n",
    "y_pred = model.predict(testX)\n",
    "plt.plot(testY, label = 'True Value')\n",
    "plt.plot(y_pred, label = 'Forecasted Value')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions using the 'sliding/rolling window approach'\n",
    "# Multi step forecast.\n",
    "# Using self predictions for making future predictions\n",
    "# Very much different from a univariate -- You will need predictions (or known) values for your independent values.\n",
    "# Nonetheles, you will follow the same steps as shown for the univariate multi-step process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X[threshold:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
